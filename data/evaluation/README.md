# Dataset Quality Evaluation

## Metrics

We evaluate the quality of the two generated datasets using three metrics:

1.  **Accuracy**: A measure of how well the paraphrase conveys the same meaning as the original text. A paraphrase that is accurate should capture the key ideas and information of the original text while maintaining the same overall meaning and context.

2.  **Fluency**: A measure of how well the paraphrase reads or sounds in the context of the original sentence or text. A paraphrase that is fluent should be grammatically correct, well-structured, and convey the same meaning as the original text, while also being easy to read or understand.

3.  **Diversity**: A measure of how different or unique the paraphrase is from the original text. A paraphrase that is diverse or original should convey the same meaning as the original text, but using different words or structures, and avoiding excessive copying or close similarity to the original text

## Data

We evaluate the quality of 200 paraphrase pairs generated by the paraphrase-mining technique and 100 paraphrase pairs generated using back translation.

Evaluated datasets:

-   [Paraphrase-mining subset, evaluated by @Matjaz12](./data_subsets/paraphrase_mining_subset0.csv)
-   [Paraphrase-mining subset, evaluated by @klaznik](./data_subsets/paraphrase_mining_subset1.csv)
-   [Paraphrase-mining subset, evaluated by @HitkoDev](./data_subsets/paraphrase_mining_subset2.csv)
-   [Backtranslation subset, evaluated by @Matjaz12](./data_subsets/backtranslation_subset0.csv)
-   [Backtranslation subset, evaluated by @klaznik](./data_subsets/backtranslation_subset1.csv)
-   [Backtranslation subset, evaluated by @HitkoDev](./data_subsets/backtranslation_subset2.csv)

A simple tool for evaluation can be accessed by running `python ./app.py [path to dataset]`.

## Analysis

A simple quality analysis is available in [Dataset Quality Analysis](./analysis.ipynb).
